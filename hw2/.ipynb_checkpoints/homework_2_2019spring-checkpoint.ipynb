{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 2:   word2vec + SVM + Evaluation\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Tuesday, February 26, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand word2vec-like term embeddings,  explore real-world challenges with SVM-based classifiers, understand and implement several evaluation metrics.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, my homework submission would be something like `555001234_hw2.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Term embeddings + SVM (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "For this homework, we will still play with Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). As in Homework 1, you'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. Additionally, this time, we also offer you the \"label\". If `label=1`, it means that this review is `Food-relevant`. If `label=0`, it means that this review is `Food-irrelevant`. Similarly, we have already done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace.\n",
    "\n",
    "There are about 40,000 reviews in total, in which about 20,000 reviews are \"Food-irrelevant\". We split the review data into two sets. *review_train.json* is the training set. *review_test.json* is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import gensim\n",
    "import dill\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please load the dataset\n",
    "# Your code below\n",
    "\n",
    "all_train_data = []\n",
    "all_test_data = []\n",
    "with open(\"review_train.json\") as f:\n",
    "    for line in f:\n",
    "        all_train_data.append(json.loads(line))\n",
    "        \n",
    "with open(\"review_test.json\") as f:\n",
    "    for line in f:\n",
    "        all_test_data.append(json.loads(line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-trained term embeddings\n",
    "\n",
    "To save your time, you can make use of  pre-trained term embeddings. In this homework, we are using one of the great pre-trained models from [GloVe](https://nlp.stanford.edu/projects/glove/) based on 2 billion tweets. GloVe is quite similar to word2vec. Unzip the *glove.6B.50d.txt.zip* file and run the code below. You will be able to load the term embeddings model, with which each word can be represented with a 50-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the pre-trained term embeddings\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    model = {line.split()[0].decode(encoding=\"utf-8\", errors=\"strict\"): np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}\n",
    "    #print(shape(model.get('the')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a vector representation for each word. First, we use the simple (arithmetic) **mean** of these vectors of words in a review to represent the review. *Note: Just ignore those words which are not in the corpus of this pre-trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please figure out the vector representation for each review in the training data and testing data.\n",
    "# Your code below\n",
    "# get the vector for each review in trainning data and correponding labels\n",
    "def vector_for_review(review, model):\n",
    "    res = np.zeros(50)\n",
    "    count = 0;\n",
    "    for word in review:\n",
    "        if word in model:\n",
    "            res += model.get(word)\n",
    "            count += 1;\n",
    "    res /= count;\n",
    "    return res;\n",
    "\n",
    "train_data_vectors = np.zeros((len(all_train_data), 50))\n",
    "train_data_labels = np.zeros(len(all_train_data))\n",
    "train_data_reviews = []\n",
    "i = 0\n",
    "for train_data in all_train_data:\n",
    "    review = train_data.get(\"review\").split()\n",
    "    train_data[\"review_score\"] = vector_for_review(review, model)\n",
    "    train_data_vectors[i] = train_data[\"review_score\"]\n",
    "    train_data_labels[i] = train_data[\"label\"]\n",
    "    train_data_reviews.append(train_data.get(\"review\"))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector for each review in test data and correponding labels\n",
    "test_data_vectors = np.zeros((len(all_test_data), 50))\n",
    "test_data_labels = np.zeros(len(all_test_data))\n",
    "test_data_reviews = []\n",
    "i = 0\n",
    "for test_data in all_test_data:\n",
    "    review = test_data.get(\"review\").split()\n",
    "    test_data[\"review_score\"] = vector_for_review(review, model)\n",
    "    test_data_vectors[i] = test_data[\"review_score\"]\n",
    "    test_data_labels[i] = test_data[\"label\"]\n",
    "    test_data_reviews.append(test_data.get(\"review\"))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "With the vector representations you get for each review, please train an SVM model to predict whether a given review is food-relevant or not. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.** You can only train your model with reviews in *review_train.json*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model training\n",
    "# Your code here\n",
    "# train the classifier using SVM\n",
    "clf = SVC(gamma=\"scale\")\n",
    "clf.fit(train_data_vectors, train_data_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to predict whether a given review is food-relevant or not. Please report the overall accuracy, precision and recall of your model on the **testing data**. You should **implement the functions for accuracy, precision, and recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def getParameters(test_labels, test_result):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for i in range(len(test_labels)):\n",
    "        if test_labels[i] == 0 and test_result[i] == 0:\n",
    "            TN += 1\n",
    "        elif test_labels[i] == 0 and test_result[i] == 1:\n",
    "            FP += 1\n",
    "        elif test_labels[i] == 1 and test_result[i] == 1:\n",
    "            TP += 1\n",
    "        elif test_labels[i] == 1 and test_result[i] == 0:\n",
    "            FN += 1\n",
    "    return TP, TN, FP, FN\n",
    "            \n",
    "    \n",
    "    \n",
    "def getResult (TP, TN, FP, FN):\n",
    "    accurancy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    return accurancy, precision, recall\n",
    "    \n",
    "    \n",
    "test_result = clf.predict(test_data_vectors)\n",
    "TP, TN, FP, FN = getParameters(test_result, test_data_labels)\n",
    "accurancy, precision, recall = getResult(TP, TN, FP, FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033557046979865\n",
      "5549 5219 396 756\n",
      "11920\n",
      "0.9033557046979865\n",
      "0.9033557046979865 0.9333894028595459 0.8800951625693894\n"
     ]
    }
   ],
   "source": [
    "print(accurancy)\n",
    "print(TP, TN, FP, FN)  #Accuracy = TP+TN/TP+FP+FN+TN\n",
    "print(len(test_result))\n",
    "print((TP + TN) / (TP + FP + FN + TN))\n",
    "print(accurancy, precision, recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-based embeddings\n",
    "\n",
    "Instead of taking the mean of term embeddings, you can directly train a **doc2vec** model for paragraph or document embeddings. You can refer to the paper [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf) for more details. And in this homework, you can make use of the implementation in [gensim](https://radimrehurek.com/gensim/models/doc2vec.html).\n",
    "\n",
    "Now, you need to:\n",
    "* Train a doc2vec model based on all reviews you have (training + testing sets).\n",
    "* Use the embeddings from your doc2vec model to represent each review and train a new SVM model.\n",
    "* Report the overall accuracy, precision and recall of your model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['divine', 'thin', 'crust', 'pizza', 'great', 'salads', 'appetizers', 'wings', 'and', 'pizza', 'cookies', 'left', 'little', 'doughy', 'with', 'ice', 'cream', 'on', 'top', 'the', 'service', 'is', 'always', 'great', 'as', 'is', 'the', 'atmosphere', 'the', 'wait', 'can', 'be', 'long', 'on', 'weekend', 'nights', 'but', 'the', 'food', 'is', 'worth', 'it', 'lunch', 'specials', 'are', 'also', 'good', 'and', 'it', 'great', 'place', 'for', 'work', 'lunch', 'large', 'patio', 'area', 'is', 'perfect', 'place', 'to', 'eat', 'most', 'of', 'the', 'year', 'my', 'go', 'to', 'menu', 'items', 'are', 'the', 'pablo', 'picasso', 'salad', 'the', 'mero', 'one', 'on', 'thin', 'crust', 'and', 'peanut', 'butter', 'pizza', 'cookie'], tags=[0.0]),\n",
       " TaggedDocument(words=['no', 'no', 'no', 'dr', 'chad', 'campbell', 'moved', 'to', 'mesa', 'from', 'pv', 'family', 'medicine', 'my', 'family', 'loved', 'him', 'he', 'was', 'just', 'rare', 'medical', 'man', 'so', 'caring', 'assume', 'he', 'is', 'that', 'far', 'away', 'because', 'of', 'non', 'compete', 'clause', 'good', 'thing', 'because', 'would', 'have', 'followed', 'him', 'if', 'it', 'was', 'closer', 'now', 'what', 'do', 'do', 'lucky', 'mesa', 'people', 'to', 'have', 'him'], tags=[1.0]),\n",
       " TaggedDocument(words=['mint', 'copies', 'of', 'essential', 'pepin', 'off', 'list', 'including', 'the', 'dvd', 'and', 'michel', 'richard', 'happy', 'in', 'the', 'kitchen', 'off', 'and', 'located', 'within', 'walking', 'distance', 'll', 'be', 'back', 'soon', 'particularly', 'to', 'dig', 'through', 'the', 'vinyl'], tags=[1.0])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a doc2vec\n",
    "# Your code here\n",
    "\n",
    "\n",
    "def read_corpus(reviews, tags):\n",
    "    for i in range(len(reviews)):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(reviews[i]), [tags[i]])\n",
    "\n",
    "\n",
    "\n",
    "train_corpus = list(read_corpus(train_data_reviews, train_data_labels))\n",
    "test_corpus = list(read_corpus(test_data_reviews, test_data_labels))\n",
    "all_corpus = train_corpus + test_corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model_test = gensim.models.doc2vec.Doc2Vec(all_corpus, vector_size=50, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['divine', 'thin', 'crust', 'pizza', 'great', 'salads', 'appetizers', 'wings', 'and', 'pizza', 'cookies', 'left', 'little', 'doughy', 'with', 'ice', 'cream', 'on', 'top', 'the', 'service', 'is', 'always', 'great', 'as', 'is', 'the', 'atmosphere', 'the', 'wait', 'can', 'be', 'long', 'on', 'weekend', 'nights', 'but', 'the', 'food', 'is', 'worth', 'it', 'lunch', 'specials', 'are', 'also', 'good', 'and', 'it', 'great', 'place', 'for', 'work', 'lunch', 'large', 'patio', 'area', 'is', 'perfect', 'place', 'to', 'eat', 'most', 'of', 'the', 'year', 'my', 'go', 'to', 'menu', 'items', 'are', 'the', 'pablo', 'picasso', 'salad', 'the', 'mero', 'one', 'on', 'thin', 'crust', 'and', 'peanut', 'butter', 'pizza', 'cookie'], tags=[0.0]), TaggedDocument(words=['no', 'no', 'no', 'dr', 'chad', 'campbell', 'moved', 'to', 'mesa', 'from', 'pv', 'family', 'medicine', 'my', 'family', 'loved', 'him', 'he', 'was', 'just', 'rare', 'medical', 'man', 'so', 'caring', 'assume', 'he', 'is', 'that', 'far', 'away', 'because', 'of', 'non', 'compete', 'clause', 'good', 'thing', 'because', 'would', 'have', 'followed', 'him', 'if', 'it', 'was', 'closer', 'now', 'what', 'do', 'do', 'lucky', 'mesa', 'people', 'to', 'have', 'him'], tags=[1.0])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "doc2vec_model.build_vocab(all_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02068353, -0.03190277,  0.02140535, -0.02938599, -0.11021171,\n",
       "        0.06279062,  0.02786241, -0.0460685 ,  0.0326471 ,  0.08625475,\n",
       "       -0.11375591, -0.14177744,  0.04788765, -0.06890019,  0.01237642,\n",
       "        0.0612667 , -0.08222976,  0.02206961,  0.00395976, -0.0859159 ,\n",
       "        0.01692121,  0.06210265, -0.06803227,  0.06234637,  0.08706392,\n",
       "        0.01956822, -0.15219447, -0.02035433,  0.01084331,  0.11902098,\n",
       "        0.07406256, -0.02338555,  0.05959496, -0.01265768,  0.02984477,\n",
       "       -0.06688582, -0.1140181 , -0.19523083, -0.03484913, -0.12728179,\n",
       "       -0.08170809, -0.09501545,  0.00152066, -0.05168055, -0.06187149,\n",
       "        0.13807724, -0.21552607,  0.17439444, -0.09575331, -0.02863809],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([\"delicious\", \"food\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = gensim.test.utils.get_tmpfile(\"my_doc2vec_model\")\n",
    "doc2vec_model.save(fname)\n",
    "doc2vec_model = Doc2Vec.load(fname)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector (datas, model):\n",
    "    vec = np.zeros((len(datas), 50))\n",
    "    i = 0\n",
    "    for data in datas:\n",
    "        review = data.get(\"review\").split()\n",
    "        data[\"doc2vec_review_score\"] = model.infer_vector(review)\n",
    "        vec[i] = data[\"doc2vec_review_score\"]\n",
    "        i += 1\n",
    "    return vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.09992659 -0.1599838  -1.15754247 -0.45520118  0.60039675  0.05038147\n",
      "  0.2751106  -0.14207143  0.56805927  0.34576863 -0.4953562  -0.50833702\n",
      " -0.09909575 -0.64252061  0.25744247 -0.47179618  0.89473879 -0.04816262\n",
      " -1.06605983 -0.30108872  0.29751068  0.79321134 -1.4038527   0.62879384\n",
      " -0.46935272 -0.2995376   0.58797115 -0.2107334  -0.18067499  0.07129885\n",
      "  0.0415977  -0.08355843 -0.71760446 -0.20409644 -0.01250778  0.01237654\n",
      "  0.46137658 -0.08277792 -0.20426697 -0.24239387 -0.56187439  0.16985391\n",
      "  0.31851715  0.11950096 -0.38217255 -0.01709676 -0.07113655  1.17250991\n",
      " -0.32318059  0.29749349] ['divine', 'thin', 'crust', 'pizza', 'great', 'salads', 'appetizers', 'wings', 'and', 'pizza', 'cookies', 'left', 'a', 'little', 'doughy', 'with', 'ice', 'cream', 'on', 'top', 'the', 'service', 'is', 'always', 'great', 'as', 'is', 'the', 'atmosphere', 'the', 'wait', 'can', 'be', 'a', 'long', 'on', 'weekend', 'nights', 'but', 'the', 'food', 'is', 'worth', 'it', 'lunch', 'specials', 'are', 'also', 'good', 'and', 'it', 's', 'a', 'great', 'place', 'for', 'a', 'work', 'lunch', 'large', 'patio', 'area', 'is', 'a', 'perfect', 'place', 'to', 'eat', 'most', 'of', 'the', 'year', 'my', 'go', 'to', 'menu', 'items', 'are', 'the', 'pablo', 'picasso', 'salad', 'the', 'n', 'mero', 'one', 'o', 'on', 'thin', 'crust', 'and', 'a', 'peanut', 'butter', 'pizza', 'cookie']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a SVM\n",
    "# Your code here\n",
    "train_data_doc2vec_vec = get_vector(all_train_data, model_test)\n",
    "clf_doc2vec = SVC(gamma=\"scale\")\n",
    "clf_doc2vec.fit(train_data_doc2vec_vec, train_data_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.69743717  0.11950138 -0.85247058 -0.24878186  0.23824167 -0.09549252\n",
      "  0.03566369  0.27571744  0.72523278  0.00428586 -0.17119852 -0.31878224\n",
      " -0.21586344 -0.16522186  0.60499001  0.01359397  0.62092298 -0.18516463\n",
      " -1.2390573  -0.09291618  0.83955616  0.79787755 -0.80167884  1.10906696\n",
      " -0.21796408 -0.30812556  0.47903466 -0.62136739  0.17538469  0.63116163\n",
      "  0.08394278  0.1312885  -0.16397946 -0.49235934  0.10070144 -0.24897605\n",
      "  0.46063909  0.10063877  0.15850802 -0.12544936 -0.5056752  -0.10727307\n",
      " -0.01939653 -0.07047641 -0.82391399 -0.46142352 -0.13532488  0.12485417\n",
      " -0.17678912  0.52859825] ['not', 'a', 'bad', 'place', 'to', 'brush', 'up', 'on', 'skills', 'or', 'just', 'go', 'to', 'practice', 'they', 'need', 'to', 'organize', 'their', 'classes', 'better', 'with', 'the', 'rotations', 'keep', 'couples', 'in', 'the', 'center', 'and', 'all', 'guests', 'in', 'a', 'circle', 'instructors', 'also', 'need', 'to', 'ensure', 'lessons', 'are', 'taught', 'at', 'the', 'appropriate', 'level', 'do', 'not', 'overwhelm', 'your', 'new', 'guests', 'better', 'than', 'fat', 'cat', 'but', 'nowhere', 'near', 'the', 'level', 'of', 'fred', 'astaire']\n"
     ]
    }
   ],
   "source": [
    "test_data_doc2vec_vec = get_vector(all_test_data, model_test)\n",
    "test_doc2vec_result = clf.predict(test_data_doc2vec_vec)\n",
    "TP, TN, FP, FN = getParameters(test_doc2vec_result, test_data_labels)\n",
    "accurancy, precision, recall = getResult(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5942 2 3 5973\n",
      "0.4986577181208054 0.9994953742640875 0.4986991187578682\n"
     ]
    }
   ],
   "source": [
    "# Report the performance\n",
    "# Your code here\n",
    "print(TP, TN, FP, FN)\n",
    "print(accurancy, precision, recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the term-based average approach vs. the doc2vec approach? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you do better?\n",
    "\n",
    "Finally, see if you can do better than either the word- or doc- based embeddings approach for classification. You may explore new features, new classifiers, etc. Whatever you like. Just provide your code and a justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: NDCG (20 points)\n",
    "\n",
    "You calculated the recall and precision in Part 1 and now you get a chance to implement NDCG. \n",
    "\n",
    "Assume that Amy searches for \"food-relevant\" reviews in the **testing set** on two search engines `A` and `B`. Since the ground-truth labels for the reviews are unknown to A and B, they need to make a prediction for each review and then return a ranked list of results based on their probabilities. The results from A are in *search_result_A.json*, and the results from B are in *search_result_B.json*. Each line contains the id of a review and its corresponding ranking.\n",
    "\n",
    "You can check their labels in *review_test.json* while calculating the NDCG scores. If a review is \"food-relevant\", the relevance score is 1. Otherwise, the relevance score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result_A = []\n",
    "search_result_B = []\n",
    "with open(\"search_result_A.json\") as f:\n",
    "    for line in f:\n",
    "        search_result_A.append(json.loads(line))\n",
    "\n",
    "with open(\"search_result_B.json\") as f:\n",
    "    for line in f:\n",
    "        search_result_B.append(json.loads(line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCG_score(rel, i):\n",
    "    # calculate the DCG Score, given rel, i\n",
    "    return rel / (math.log2(i + 1));\n",
    "\n",
    "def id_rel(datas):\n",
    "    # mapping the id with label\n",
    "    id_rel = {}\n",
    "    for data in datas:\n",
    "        id_rel[data.get(\"id\")] = data.get(\"label\")\n",
    "    return id_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id2rel = id_rel(all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG for search_result_A.json\n",
    "# Your code here\n",
    "DCG_A = 0\n",
    "for result in search_result_A:\n",
    "    cur_id = result.get(\"id\")\n",
    "    rel = test_id2rel.get(cur_id)\n",
    "    DCG_A += DCG_score(rel, result.get(\"rank\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG for search_result_B.json\n",
    "# Your code here\n",
    "DCG_B = 0\n",
    "for result in search_result_B:\n",
    "    cur_id = result.get(\"id\")\n",
    "    rel = test_id2rel.get(cur_id)\n",
    "    DCG_B += DCG_score(rel, result.get(\"rank\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
